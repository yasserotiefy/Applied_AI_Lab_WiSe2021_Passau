{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASY11viGC-CR"
   },
   "source": [
    "### Environment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 113094,
     "status": "ok",
     "timestamp": 1636221066554,
     "user": {
      "displayName": "Yasser Otiefy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgH8nT7s1o73eNNJPavJleQKJePFH0HS80Pv_-tmQ=s64",
      "userId": "11606645386995589698"
     },
     "user_tz": -60
    },
    "id": "AQD3X56MDC41",
    "outputId": "98a56160-961f-40f1-c157-9853b2ad57b8"
   },
   "outputs": [],
   "source": [
    "# !pip install swifter pandas numpy\n",
    "# !pip install -q -U keras-tuner\n",
    "# !pip install tensorflow-gpu keras keras-transformer keras_bert\n",
    "# !pip install pydot pydotplus\n",
    "# !pip install h5py==2.10.0 --force-reinstall\n",
    "# !apt-get install graphviz -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8GPhl_YBO9X"
   },
   "source": [
    "### Download required embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpdWSjMNR18V"
   },
   "outputs": [],
   "source": [
    "# !wget https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip data/\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip data/\n",
    "# !wget https://nlp.stanford.edu/data/glove.840B.300d.zip data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI8SduklTKXO"
   },
   "outputs": [],
   "source": [
    "# !unzip data/uncased_L-12_H-768_A-12.zip \n",
    "# !unzip data/crawl-300d-2M.vec.zip\n",
    "# !unzip data/glove.840B.300d.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp6skypuClUf"
   },
   "source": [
    "### Run Preprocessing File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnDeHDjqCqPX"
   },
   "outputs": [],
   "source": [
    "# !python preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fh7BB7hTBKOs"
   },
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 521,
     "status": "ok",
     "timestamp": 1636209799226,
     "user": {
      "displayName": "Yasser Otiefy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgH8nT7s1o73eNNJPavJleQKJePFH0HS80Pv_-tmQ=s64",
      "userId": "11606645386995589698"
     },
     "user_tz": -60
    },
    "id": "qtsQ8SovAr4v",
    "outputId": "cec16143-f0a5-4dc7-b4e6-85204f1e9bfb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import choice, seed\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, CuDNNLSTM as LSTM, Dropout, BatchNormalization\n",
    "from keras.layers import Dense, Concatenate, Embedding, Bidirectional, Lambda, Conv1D\n",
    "from keras.layers import Add, TimeDistributed, GlobalMaxPooling1D\n",
    "from tensorflow.compat.v1.keras.optimizers import Adam, Nadam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import Callback\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import keras_tuner as kt\n",
    "import keras\n",
    "from keras_bert.loader import load_trained_model_from_checkpoint\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras_bert import get_custom_objects\n",
    "from keras_bert import Tokenizer\n",
    "from collections import defaultdict\n",
    "from eval import read_submission, get_ndcg\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "import dask.dataframe as dd\n",
    "import joblib\n",
    "\n",
    "\n",
    "BERT_PRETRAINED_DIR = \"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/data/uncased_L-12_H-768_A-12\"\n",
    "VAL_ANS_PATH = '/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/data/valid_answer.json'\n",
    "LABEL_PATH = '/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/data/multimodal_labels.txt'\n",
    "\n",
    "MAX_EPOCH = 20\n",
    "MAX_LEN = 10\n",
    "B_SIZE = 256\n",
    "FOLD_IDS = [-1]\n",
    "FOLD_NUM = 20\n",
    "THRE = 0.5\n",
    "SHUFFLE = True\n",
    "MAX_BOX = 5\n",
    "MAX_CHAR = 5\n",
    "PREFIX = \"[image-bert-concat-query]-wwm_uncased_L12-768_v3_1M_example\"\n",
    "SEED = 2021\n",
    "ACCUM_STEP = int(128 // B_SIZE)\n",
    "SAVE_EPOCHS=[10, 20, 35, 50, 80, 100]\n",
    "IMAGE_LABEM_CONCAT_TOKEN = \"###\"\n",
    "CONCAT_TOKE = \"[unused0]\"\n",
    "\n",
    "cfg = {}\n",
    "cfg[\"verbose\"] = PREFIX\n",
    "cfg[\"base_dir\"] = BERT_PRETRAINED_DIR\n",
    "cfg['maxlen'] = MAX_LEN\n",
    "cfg[\"max_box\"] = MAX_BOX\n",
    "cfg[\"max_char\"] = MAX_CHAR\n",
    "cfg[\"lr\"] = 1e-4\n",
    "cfg['min_lr'] = 6e-8\n",
    "cfg[\"opt\"] = \"nadam\"\n",
    "cfg[\"loss_w\"] =  20.\n",
    "cfg[\"trainable\"] = True\n",
    "cfg[\"bert_trainable\"] = True\n",
    "cfg[\"mix_mode\"] = \"\"   # add concat average\n",
    "cfg[\"unit1_1\"] = 128\n",
    "cfg[\"accum_step\"] = ACCUM_STEP\n",
    "cfg[\"cls_num\"] = 2\n",
    "cfg[\"raw_filename\"] = \"{}_{}oof{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rqxc8sS8Ar43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'baby products', 'furniture', 'others', 'hair', 'underwear', 'makeup, perfume, beauty tools and essential oils', 'storage supplies', 'sporting goods', 'household fabric', 'home decoration', 'home / personal cleaning tools', 'household electrical appliances', 'bed linens', 'clothes (accessories, baby clothing, etc.)', 'kitchenware', 'stationery', 'hand', 'skirt & dress', 'digital supplies', 'human face', 'arm', 'outdoor product', 'luggage, leather goods', 'top clothes (coat, jacket, shirt, etc.)', 'shoes', 'snacks, nuts, liquor and tea', 'bottom clothes (trousers, pants, etc.)', 'motorcycle, motorcycle accessories, vehicles, bicycle and riding equipment', 'personal care', 'lighting', 'toys', 'accessories (jewelry, clothing accessories, belts, hats, scarves, etc.)', 'bottle drink'}\n"
     ]
    }
   ],
   "source": [
    "def get_vocab():\n",
    "    \n",
    "    if \"albert\"in cfg[\"verbose\"].lower():\n",
    "        dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab_chinese.txt')\n",
    "    else:\n",
    "        dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "    with open(dict_path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [l.strip() for l in lines]\n",
    "\n",
    "    word_index = {v: k  for k, v in enumerate(lines)}\n",
    "    return word_index\n",
    "\n",
    "\n",
    "word_index = get_vocab()\n",
    "cfg[\"x_pad\"] = word_index[\"[PAD]\"]\n",
    "print(cfg['x_pad'])\n",
    "tokenizer = Tokenizer(word_index)\n",
    "\n",
    "\n",
    "def get_label(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        label2id = {l.split('\\n')[0].split('\\t')[1]:int(l.split('\\n')[0].split('\\t')[0]) for l in lines[1:]}\n",
    "        id2label = {int(l.split('\\n')[0].split('\\t')[0]):l.split('\\n')[0].split('\\t')[1] for l in lines[1:]}\n",
    "    return label2id, id2label\n",
    "\n",
    "\n",
    "label2id, id2label = get_label(LABEL_PATH)\n",
    "label_set = set(label2id.keys())\n",
    "print(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/train_data.pkl', 'rb') as outp:\n",
    "#     train_data= joblib.load(outp)\n",
    "# 100K sample\n",
    "with open('data/1M_data.pkl', 'rb') as outp:\n",
    "    train_data = joblib.load(outp)\n",
    "\n",
    "with open(\"data/sample_val.pkl\", \"rb\") as f:\n",
    "    val_data = joblib.load(f)\n",
    "\n",
    "# with open('data/val_data.pkl', 'rb') as outp:\n",
    "#     val_data = pickle.load(outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pMZVTqarAr46"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=np.float16)\n",
    "\n",
    "\n",
    "def load_embed(path, dim=300, word_index=None):\n",
    "    embedding_index = {}\n",
    "    with open(path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            l = l.strip().split()\n",
    "            word, arr = l[0], l[1:]\n",
    "            if len(arr) != dim:\n",
    "                print(\"[!] l = {}\".format(l))\n",
    "                continue\n",
    "            if word_index and word not in word_index:\n",
    "                continue\n",
    "            word, arr = get_coefs(word, arr)\n",
    "            embedding_index[word] = arr\n",
    "    return embedding_index\n",
    "\n",
    "\n",
    "def build_matrix(path, word_index=None, max_features=None, dim=300):\n",
    "    embedding_index = load_embed(path, dim=dim, word_index=word_index)\n",
    "    max_features = len(word_index) + 1 if max_features is None else max_features \n",
    "    embedding_matrix = np.zeros((max_features + 1, dim))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "                unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_word_embed(word_embed_glove=\"data/glove.840B.300d.txt\", \n",
    "                    word_embed_crawl=\"data/crawl-300d-2M.vec\",\n",
    "               save_filename=\"word_embedding_matrix\",\n",
    "               word_index=None):\n",
    "    \"\"\"\n",
    "    (30524, 300) 7590\n",
    "    (30524, 300) 7218\n",
    "    \"\"\"    \n",
    "    if os.path.exists(save_filename + \".npy\"):\n",
    "        word_embedding_matrix = np.load(save_filename + \".npy\").astype(\"float32\")\n",
    "    else:\n",
    "        word_embedding_matrix, _ = build_matrix(word_embed_glove, word_index=word_index, dim=300)\n",
    "        word_embedding_matrix_v2, _ = build_matrix(word_embed_crawl, word_index=word_index, dim=300)\n",
    "        word_embedding_matrix = np.concatenate([word_embedding_matrix, word_embedding_matrix_v2], axis=1)\n",
    "        \n",
    "        gc.collect()\n",
    "        np.save(save_filename, word_embedding_matrix)\n",
    "    return word_embedding_matrix\n",
    "\n",
    "\n",
    "word_embedding_matrix = load_word_embed(word_index=word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30524, 600)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tHoD4ogMAr47"
   },
   "outputs": [],
   "source": [
    "def build_model(cfg, summary=False, word_embedding_matrix=None):\n",
    "    \n",
    "    def _get_model(base_dir, cfg_=None):\n",
    "        config_file = os.path.join(base_dir, 'bert_config.json')\n",
    "        checkpoint_file = os.path.join(base_dir, 'bert_model.ckpt')\n",
    "        if not os.path.exists(config_file):\n",
    "            config_file = os.path.join(base_dir, 'bert_config_large.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'roberta_l24_large_model')\n",
    "        # print(config_file, checkpoint_file)\n",
    "        model = load_trained_model_from_checkpoint(config_file, \n",
    "                                           checkpoint_file, \n",
    "                                           training=False, \n",
    "                                           trainable=cfg_[\"bert_trainable\"], \n",
    "                                           output_layer_num=cfg[\"cls_num\"],\n",
    "                                           seq_len=None)\n",
    "        return model\n",
    "    \n",
    "    def get_opt(num_example, warmup_proportion=0.1, lr=2e-5, min_lr=None):\n",
    "        if cfg[\"opt\"].lower() == \"nadam\":\n",
    "            opt = Nadam(lr=lr)\n",
    "        else:\n",
    "            total_steps, warmup_steps = calc_train_steps(\n",
    "                num_example=num_example,\n",
    "                batch_size=B_SIZE,\n",
    "                epochs=MAX_EPOCH,\n",
    "                warmup_proportion=warmup_proportion,\n",
    "            )\n",
    "\n",
    "            opt = AdamWarmup(total_steps, warmup_steps, lr=lr, min_lr=min_lr)\n",
    "\n",
    "        return opt\n",
    "\n",
    "    model1 = _get_model(cfg[\"base_dir\"], cfg)\n",
    "    model1 = Model(inputs=model1.inputs[: 2], outputs=model1.layers[-7].output)\n",
    "\n",
    "    if word_embedding_matrix is not None:\n",
    "        embed_layer = Embedding(input_dim=word_embedding_matrix.shape[0], \n",
    "                                output_dim=word_embedding_matrix.shape[1],\n",
    "                                weights=[word_embedding_matrix],\n",
    "                                trainable=cfg[\"trainable\"],\n",
    "                                name=\"embed_layer\"\n",
    "                         )\n",
    "        \n",
    "    inp_token1 = Input(shape=(None, ), dtype=np.int32, name=\"query_token_input\")\n",
    "    inp_segm1 = Input(shape=(None, ), dtype=np.float32, name=\"query_segm_input\")\n",
    "      \n",
    "    \n",
    "    inp_image = Input(shape=(None, 2048), dtype=np.float32, name=\"image_input\")\n",
    "    inp_image_mask = Input(shape=(None, ), dtype=np.float32, name=\"image_mask_input\")\n",
    "    inp_pos = Input(shape=(None, 5), dtype=np.float32, name=\"image_pos_input\")        \n",
    "    inp_image_char = Input(shape=(None, cfg[\"max_char\"]), dtype=np.int32, name='image_char_input')\n",
    "    \n",
    "    \n",
    "    mask = Lambda(lambda x: K.cast(K.not_equal(x, cfg[\"x_pad\"]), 'float32'), name=\"token_mask\")(inp_token1)\n",
    "    word_embed = embed_layer(inp_token1)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "    word_embed = Bidirectional(LSTM(cfg[\"unit1_1\"], return_sequences=True), merge_mode=\"sum\")(word_embed)\n",
    "    word_embed = BatchNormalization()(word_embed)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "\n",
    "    sequence_output = model1([inp_token1, inp_segm1])\n",
    "    sequence_output = Concatenate(axis=-1)([sequence_output, word_embed])\n",
    "    text_pool = Lambda(lambda x: x[:, 0, :])(sequence_output)\n",
    "\n",
    "    # Share weights of character-level embedding for premise and hypothesis\n",
    "    character_embedding_layer = TimeDistributed(Sequential([\n",
    "        embed_layer,\n",
    "        # Embedding(input_dim=100, output_dim=char_embedding_size, input_length=chars_per_word),\n",
    "        Conv1D(filters=128, kernel_size=3, name=\"char_embed_conv1d\"),\n",
    "        GlobalMaxPooling1D()\n",
    "    ]), name='CharEmbedding')\n",
    "    character_embedding_layer.build(input_shape=(None, None, cfg[\"max_char\"]))\n",
    "    image_char_embed  = character_embedding_layer(inp_image_char)    \n",
    "    image_embed = Concatenate(axis=-1)([image_char_embed, inp_image])    \n",
    "    image_embed = Dense(256, activation='relu', name='image_embed')(image_embed)\n",
    "    image_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([image_embed, inp_image_mask])\n",
    "    pos_embed = Dense(256, activation='relu', name='pos_embed')(inp_pos)\n",
    "    pos_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([pos_embed, inp_image_mask])\n",
    "    embed = Add()([image_embed , pos_embed]) # batch, maxlen(10), 1024+128\n",
    "    \n",
    "    image_embed = Bidirectional(LSTM(512, return_sequences=True), merge_mode=\"sum\")(embed)\n",
    "    image_embed = BatchNormalization()(image_embed)\n",
    "    image_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([image_embed, inp_image_mask])\n",
    "    \n",
    "    image_pool = Lambda(lambda x: x[:, 0, :])(image_embed)\n",
    "    \n",
    "    pool = Concatenate(axis=-1)([image_pool, text_pool])\n",
    "    pool = Dense(1024, activation=\"relu\")(pool)\n",
    "    pool = Dropout(0.3)(pool)\n",
    "    pool = Dense(512, activation=\"relu\")(pool)\n",
    "    pool = Dense(128, activation=\"relu\")(pool)\n",
    "    \n",
    "    output = Dense(2, activation='softmax', name='output')(pool)\n",
    "\n",
    "    opt = get_opt(num_example=cfg[\"num_example\"], lr=cfg['lr'], min_lr=cfg['min_lr'])\n",
    "    model = Model(inputs=[inp_token1, inp_segm1, \n",
    "                          inp_image, inp_image_mask,\n",
    "                          inp_pos, inp_image_char], outputs=[output])#\n",
    "    \n",
    "    model.compile(optimizer=opt, loss={\n",
    "                'output': 'sparse_categorical_crossentropy'\n",
    "            }, metrics=['accuracy'])\n",
    "    if summary:\n",
    "        model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_model(cfg, summary=False, word_embedding_matrix=None):\n",
    "    \n",
    "    def _get_model(base_dir, cfg_=None):\n",
    "        config_file = os.path.join(base_dir, 'bert_config.json')\n",
    "        checkpoint_file = os.path.join(base_dir, 'bert_model.ckpt')\n",
    "        if not os.path.exists(config_file):\n",
    "            config_file = os.path.join(base_dir, 'bert_config_large.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'roberta_l24_large_model')\n",
    "        # print(config_file, checkpoint_file)\n",
    "        model = load_trained_model_from_checkpoint(config_file, \n",
    "                                           checkpoint_file, \n",
    "                                           training=False, \n",
    "                                           trainable=cfg_[\"bert_trainable\"], \n",
    "                                           output_layer_num=cfg[\"cls_num\"],\n",
    "                                           seq_len=None)\n",
    "        return model\n",
    "    \n",
    "    def get_opt(num_example, warmup_proportion=0.1, lr=2e-5, min_lr=None):\n",
    "        if cfg[\"opt\"].lower() == \"nadam\":\n",
    "            opt = Nadam(lr=lr)\n",
    "        else:\n",
    "            total_steps, warmup_steps = calc_train_steps(\n",
    "                num_example=num_example,\n",
    "                batch_size=B_SIZE,\n",
    "                epochs=MAX_EPOCH,\n",
    "                warmup_proportion=warmup_proportion,\n",
    "            )\n",
    "\n",
    "            opt = AdamWarmup(total_steps, warmup_steps, lr=lr, min_lr=min_lr)\n",
    "\n",
    "        return opt\n",
    "\n",
    "    # model1 = _get_model(cfg[\"base_dir\"], cfg)\n",
    "    # model1 = Model(inputs=model1.inputs[: 2], outputs=model1.layers[-7].output)\n",
    "\n",
    "    global word_index\n",
    "    word_embedding_matrix = load_word_embed(word_index=word_index)\n",
    "    embed_layer = Embedding(input_dim=word_embedding_matrix.shape[0], \n",
    "                            output_dim=word_embedding_matrix.shape[1],\n",
    "                            weights=[word_embedding_matrix],\n",
    "                            trainable=cfg[\"trainable\"],\n",
    "                            name=\"embed_layer\"\n",
    "                        )\n",
    "        \n",
    "    inp_token1 = Input(shape=(None, ), dtype=np.int32, name=\"query_token_input\")\n",
    "    inp_segm1 = Input(shape=(None, ), dtype=np.float32, name=\"query_segm_input\")\n",
    "    \n",
    "#     inp_token2 = Input(shape=(None, ), dtype=np.int32)\n",
    "#     inp_segm2 = Input(shape=(None, ), dtype=np.float32)    \n",
    "    \n",
    "    inp_image = Input(shape=(None, 2048), dtype=np.float32, name=\"image_input\")\n",
    "    inp_image_mask = Input(shape=(None, ), dtype=np.float32, name=\"image_mask_input\")\n",
    "    inp_pos = Input(shape=(None, 5), dtype=np.float32, name=\"image_pos_input\")        \n",
    "    inp_image_char = Input(shape=(None, cfg[\"max_char\"]), dtype=np.int32, name='image_char_input')\n",
    "    \n",
    "    \n",
    "    mask = Lambda(lambda x: K.cast(K.not_equal(x, cfg[\"x_pad\"]), 'float32'), name=\"token_mask\")(inp_token1)\n",
    "    word_embed = embed_layer(inp_token1)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "    \n",
    "    # hp_units_lstm = hp.Int('lstm_units1', min_value=64, max_value=512, step=32)\n",
    "    word_embed = Bidirectional(LSTM(cfg[\"unit1_1\"], return_sequences=True), merge_mode=\"sum\")(word_embed)\n",
    "    word_embed = BatchNormalization()(word_embed)\n",
    "    # word_embed = Dropout(0.3)(word_embed)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "\n",
    "    # sequence_output = model1([inp_token1, inp_segm1])\n",
    "    # sequence_output = Concatenate(axis=-1)([sequence_output, word_embed])\n",
    "    text_pool = Lambda(lambda x: x[:, 0, :])(word_embed)\n",
    "\n",
    "\n",
    "    # Share weights of character-level embedding for premise and hypothesis\n",
    "    character_embedding_layer = TimeDistributed(Sequential([\n",
    "        embed_layer,\n",
    "        # Embedding(input_dim=100, output_dim=char_embedding_size, input_length=chars_per_word),\n",
    "        Conv1D(filters=128, kernel_size=3, name=\"char_embed_conv1d\"),\n",
    "        GlobalMaxPooling1D()\n",
    "    ]), name='CharEmbedding')\n",
    "    character_embedding_layer.build(input_shape=(None, None, cfg[\"max_char\"]))\n",
    "    image_char_embed  = character_embedding_layer(inp_image_char)    \n",
    "    image_embed = Concatenate(axis=-1)([image_char_embed, inp_image])    \n",
    "    image_embed = Dense(256, activation='relu', name='image_embed')(image_embed)\n",
    "    image_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([image_embed, inp_image_mask])\n",
    "    pos_embed = Dense(256, activation='relu', name='pos_embed')(inp_pos)\n",
    "    pos_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([pos_embed, inp_image_mask])\n",
    "    embed = Add()([image_embed , pos_embed]) # batch, maxlen(10), 1024+128\n",
    "    \n",
    "    image_embed = Bidirectional(LSTM(512, return_sequences=True), merge_mode=\"sum\")(embed)\n",
    "    image_embed = BatchNormalization()(image_embed)\n",
    "    image_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([image_embed, inp_image_mask])\n",
    "    \n",
    "    image_pool = Lambda(lambda x: x[:, 0, :])(image_embed)\n",
    "    \n",
    "    pool = Concatenate(axis=-1)([image_pool, text_pool])\n",
    "    pool = Dense(1024, activation=\"relu\")(pool)\n",
    "    pool = Dropout(0.3)(pool)\n",
    "    pool = Dense(512, activation=\"relu\")(pool)\n",
    "    pool = Dense(128, activation=\"relu\")(pool)\n",
    "    \n",
    "    output = Dense(2, activation='softmax', name='output')(pool)\n",
    "\n",
    "    opt = get_opt(num_example=cfg[\"num_example\"], lr=cfg['lr'], min_lr=cfg['min_lr'])\n",
    "    model = Model(inputs=[inp_token1, inp_segm1, \n",
    "                          inp_image, inp_image_mask,\n",
    "                          inp_pos, inp_image_char], outputs=[output])#\n",
    "    \n",
    "    model.compile(optimizer=opt, loss={\n",
    "                'output': 'sparse_categorical_crossentropy'\n",
    "            }, metrics=['accuracy'])\n",
    "    if summary:\n",
    "        model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dWGkWTgbAr48"
   },
   "outputs": [],
   "source": [
    "def token2id_X(X, x_dict, maxlen=None):\n",
    "    x = tokenizer.tokenize(X)\n",
    "    if maxlen:\n",
    "        x = x[: 1] + list(x)[1: maxlen - 1] + x[-1: ]     \n",
    "    seg = [0 for _ in x]\n",
    "    token = list(x)\n",
    "    x = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in token]\n",
    "    assert len(x) == len(seg)\n",
    "    return x, seg\n",
    "\n",
    "\n",
    "def seq_padding(X, maxlen=None, padding_value=None, debug=False):\n",
    "    L = [len(x) for x in X]\n",
    "    if maxlen is None:\n",
    "        maxlen = max(L)\n",
    "\n",
    "    pad_X = np.array([\n",
    "        np.concatenate([x, [padding_value] * (maxlen - len(x))]) if len(x) < maxlen else x[: maxlen] for x in X\n",
    "    ])\n",
    "    if debug:\n",
    "        print(\"[!] before pading {}\\n\".format(X))\n",
    "        print(\"[!] after pading {}\\n\".format(pad_X))\n",
    "    return pad_X\n",
    "    \n",
    "\n",
    "def MyChoice(Myset):\n",
    "    result = []\n",
    "    for i in Myset:\n",
    "        temp_set = set()\n",
    "        temp_set.add(i)\n",
    "        cho = choice(list(Myset - temp_set))\n",
    "        result.append(cho)\n",
    "    return result\n",
    "\n",
    "\n",
    "class data_generator:\n",
    "    \n",
    "    def __init__(self, data, batch_size=B_SIZE, shuffle=SHUFFLE):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        inp_token1,\n",
    "        inp_segm1,\n",
    "        inp_image,\n",
    "        inp_image_mask,\n",
    "        inp_pos, \n",
    "        inp_image_char\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(idxs)\n",
    "            T1, T2, Image1, Pos1, label_word_list, image1_mask, image1_char = [], [], [], [], [], [], []\n",
    "            S1, S2, Image2, Pos2, image2_mask, image2_char = [], [], [], [], [], [] # 负样本\n",
    "            Id_set = set()\n",
    "\n",
    "            for i in idxs:\n",
    "                d = self.data.iloc[i]\n",
    "                text = d['words']\n",
    "                label_words = d['label_words']\n",
    "                \n",
    "                t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "                image = np.array(d['features'], dtype=\"float32\")\n",
    "                image = image[: cfg[\"max_box\"]]\n",
    "                img_mask = [1 for _ in image[: cfg[\"max_box\"]]]\n",
    "                \n",
    "                pos = np.array(d['pos'], dtype=\"float32\")\n",
    "                pos = pos[: cfg[\"max_box\"]]\n",
    "                \n",
    "                image_char = [token2id_X(ent, x_dict=word_index)[0] for ent in label_words.split(IMAGE_LABEM_CONCAT_TOKEN)]\n",
    "                image_char = image_char[: cfg[\"max_box\"]]\n",
    "                # print(\"image_char\", len(image_char))\n",
    "                image_char = pad_sequences(image_char, \n",
    "                                           maxlen=cfg[\"max_char\"], \n",
    "                                           dtype='int32',\n",
    "                                           padding='post',\n",
    "                                           truncating='post',\n",
    "                                           value=cfg[\"x_pad\"])\n",
    "                \n",
    "                assert image.shape[0] == pos.shape[0]\n",
    "                assert image.shape[0] == cfg[\"max_box\"] or image.shape[0] == len(label_words.split(IMAGE_LABEM_CONCAT_TOKEN))\n",
    "                assert image_char.shape == (image.shape[0], cfg[\"max_char\"])\n",
    "\n",
    "                T1.append(t1)\n",
    "                T2.append(t2)\n",
    "                Image1.append(image)\n",
    "                image1_mask.append(img_mask)  \n",
    "                Pos1.append(pos)\n",
    "                image1_char.append(image_char)\n",
    "                Id_set.add(i)\n",
    "\n",
    "                if len(T1) == self.batch_size//2 or i == idxs[-1]:\n",
    "                    ## 加入负样本\n",
    "                    Id_new = MyChoice(Id_set)\n",
    "#                     print(Id_set, Id_new)\n",
    "                    for i, id_ in enumerate(Id_new):\n",
    "                        d_new = self.data.iloc[id_]\n",
    "                        text = d_new['words']\n",
    "                        t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "                        S1.append(t1)\n",
    "                        S2.append(t2)\n",
    "                        \n",
    "                        image = Image1[i]\n",
    "                        img_mask = image1_mask[i]\n",
    "                        pos = Pos1[i]\n",
    "                        image_char = image1_char[i]\n",
    "                        \n",
    "                        Image2.append(image)\n",
    "                        Pos2.append(pos)\n",
    "                        image2_mask.append(img_mask)\n",
    "                        image2_char.append(image_char)\n",
    "                    \n",
    "                    Y = [1] * len(T1) + [0] * len(S1)\n",
    "                   \n",
    "                    T1 = seq_padding(T1 + S1, padding_value=cfg[\"x_pad\"]) \n",
    "                    T2 = seq_padding(T2 + S2, padding_value=cfg[\"x_pad\"])\n",
    "                    \n",
    "                    Image1 = seq_padding(Image1 + Image2, \n",
    "                                         padding_value=np.zeros(shape=(2048, ))\n",
    "                                        )\n",
    "                                                         \n",
    "                    Pos1 = seq_padding(Pos1 + Pos2,\n",
    "                                       padding_value=np.zeros(shape=(5, ))\n",
    "                                      )\n",
    "                    image1_mask = seq_padding(image1_mask + image2_mask,\n",
    "                                             padding_value=0)\n",
    "                    \n",
    "                    image1_char = seq_padding(image1_char + image2_char,\n",
    "                                             padding_value=np.zeros(shape=(cfg[\"max_char\"])), debug=False)\n",
    "                    \n",
    "                    Y = np.array(Y).reshape((len(T1), -1))\n",
    "                    \n",
    "                    idx = np.arange(len(T1))\n",
    "                    np.random.shuffle(idx)\n",
    "        \n",
    "                    T1 = T1[idx]\n",
    "                    T2 = T2[idx]\n",
    "                    Image1 = Image1[idx]\n",
    "                    image1_mask = image1_mask[idx]\n",
    "                    Pos1 = Pos1[idx]\n",
    "                    image1_char = image1_char[idx]\n",
    "                    Y = Y[idx]\n",
    "                    \n",
    "                    yield [T1, T2, Image1, image1_mask, Pos1, image1_char], Y\n",
    "                    T1, T2, Image1, Pos1, label_word_list, image1_mask, image1_char = [], [], [], [], [], [], []\n",
    "                    S1, S2, Image2, Pos2, image2_mask, image2_char = [], [], [], [], [], [] # 负样本\n",
    "                    Id_set = set()\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cSFjbRg6Ar5A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n",
      "x (256, 10) (256, 10) (256, 5, 2048) (256, 5) (256, 5, 5) (256, 5, 5) (256, 1)\n"
     ]
    }
   ],
   "source": [
    "train_D = data_generator(train_data)\n",
    "val_D = data_generator(val_data)\n",
    "_i  = 0\n",
    "for d in train_D:\n",
    "    _i += 1\n",
    "    if  _i > 10:\n",
    "        break\n",
    "    print('x',d[0][0].shape, d[0][1].shape,d[0][2].shape, d[0][3].shape, d[0][4].shape, d[0][5].shape, d[1].shape)\n",
    "\n",
    "_i  = 0\n",
    "for d in val_D:\n",
    "    _i += 1\n",
    "    if  _i > 10:\n",
    "        break\n",
    "    print('x',d[0][0].shape, d[0][1].shape,d[0][2].shape, d[0][3].shape, d[0][4].shape, d[0][5].shape, d[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wK6_y8irAr5C"
   },
   "outputs": [],
   "source": [
    "class Evaluate(Callback):\n",
    "    def __init__(self, filename=None):\n",
    "        self.score = []\n",
    "        self.best = 0.\n",
    "        self.filename = filename\n",
    "       \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch ==  0:\n",
    "            print(\"[!] test load&save model\")\n",
    "            f = self.filename + \".h5\"\n",
    "            custom_objects = get_custom_objects()\n",
    "            self.model.save(f, include_optimizer=False, overwrite=True)\n",
    "            if \"bert\" in cfg[\"verbose\"]:\n",
    "                model_ = load_model(f, custom_objects=custom_objects)  \n",
    "            else:\n",
    "                model_ = load_model(f) \n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "#         if epoch + 1 < 5:\n",
    "#             return\n",
    "        score = self.evaluate(self.model)\n",
    "        self.score.append((epoch, score))\n",
    "        logs['nDCG@5'] = score\n",
    "        tf.summary.scalar('nDCG@5', score, step=epoch)\n",
    "        \n",
    "        if epoch + 1 in SAVE_EPOCHS:\n",
    "            self.model.save(self.filename + \"_{}.h5\".format(epoch + 1), include_optimizer=False, overwrite=True)             \n",
    "        if score > self.best:\n",
    "            self.model.save(self.filename + \".h5\", include_optimizer=False)\n",
    "            \n",
    "        if score > self.best:\n",
    "            self.best = score\n",
    "            print(\"[!] epoch = {}, new NDCG best score = {}\".format(epoch + 1,  score))\n",
    "        print('[!] epoch = {}, score = {}, NDCG best score: {}\\n'.format(epoch + 1, score, self.best))\n",
    "\n",
    "    def eval_preprocess(self, row):\n",
    "\n",
    "            d = row\n",
    "            text = d['query']\n",
    "            label_words = d['label_words']\n",
    "            t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "            \n",
    "            image = np.array(d['feature_convert'], dtype=\"float32\")\n",
    "            image = image[: cfg[\"max_box\"]]\n",
    "            img_mask = [1 for _ in image[: cfg[\"max_box\"]]]                   \n",
    "            pos = np.array(d['pos'], dtype=\"float32\")\n",
    "            pos = pos[: cfg[\"max_box\"]]\n",
    "            \n",
    "            image_char = [token2id_X(ent, x_dict=word_index)[0] for ent in label_words.split(IMAGE_LABEM_CONCAT_TOKEN)]\n",
    "            image_char = image_char[: cfg[\"max_box\"]]\n",
    "            image_char = pad_sequences(image_char, \n",
    "                                       maxlen=cfg[\"max_char\"], \n",
    "                                       dtype='int32',\n",
    "                                       padding='post',\n",
    "                                       truncating='post',\n",
    "                                       value=cfg[\"x_pad\"])\n",
    "            output = self.model.predict([np.asarray([t1]), np.asarray([t2]), np.asarray([image]), np.asarray([img_mask]), np.asarray([pos]), np.asarray([image_char])])\n",
    "            return output\n",
    "\n",
    "\n",
    "    def evaluate(self, model):\n",
    "        self.model = model\n",
    "        result = defaultdict(list)\n",
    "        val_results = val_data.apply(self.eval_preprocess, axis=1)\n",
    "        qid = val_data[\"query_id\"].values\n",
    "        pid = val_data[\"product_id\"].values\n",
    "\n",
    "\n",
    "        for i in trange(len(val_data)): \n",
    "            result[qid[i]].append((pid[i], val_results[i][0][1]))\n",
    "            \n",
    "        query_id,product1,product2,product3,product4,product5 = [],[],[],[],[],[]\n",
    "        for key in result.keys():\n",
    "            rlist = result[key]\n",
    "            rlist.sort(key=lambda x: x[1], reverse=True)\n",
    "            query_id.append(key)\n",
    "            product1.append(rlist[0][0])\n",
    "            product2.append(rlist[1][0])\n",
    "            product3.append(rlist[2][0])\n",
    "            product4.append(rlist[3][0])\n",
    "            product5.append(rlist[4][0])\n",
    "        sub = pd.DataFrame({'query-id':query_id,\n",
    "                            'product1':product1,\n",
    "                            'product2':product2,\n",
    "                            'product3':product3,\n",
    "                            'product4':product4,\n",
    "                            'product5':product5,\n",
    "\n",
    "        })\n",
    "        sub.to_csv('result/val_submission.csv',index=0)\n",
    "        \n",
    "        reference = json.load(open(VAL_ANS_PATH))\n",
    "        \n",
    "        # read predictions\n",
    "        k = 5\n",
    "        predictions = read_submission('result/val_submission.csv', reference, k)\n",
    "\n",
    "        # compute score for each query\n",
    "        score_sum = 0.\n",
    "        for qid in reference.keys():\n",
    "            ground_truth_ids = set([str(pid) for pid in reference[qid]])\n",
    "            ref_vec = [1.0] * len(ground_truth_ids)\n",
    "            pred_vec = [1.0 if pid in ground_truth_ids else 0.0 for pid in predictions[qid]]\n",
    "            score_sum += get_ndcg(pred_vec, ref_vec, k)\n",
    "        # the higher score, the better\n",
    "        score = score_sum / len(reference)\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0U5XAi6Ar5D"
   },
   "source": [
    "## Train Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10148921,
     "status": "ok",
     "timestamp": 1636219971941,
     "user": {
      "displayName": "Yasser Otiefy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgH8nT7s1o73eNNJPavJleQKJePFH0HS80Pv_-tmQ=s64",
      "userId": "11606645386995589698"
     },
     "user_tz": -60
    },
    "id": "-8khJgNZAr5F",
    "outputId": "a957f6b0-64b7-4ed2-98e0-788e46c9bf66",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n",
    "fold_id = -1\n",
    "print(\"\\n\\n[!] fold_id = {} starting\".format(fold_id))\n",
    "# cfg[\"filename\"] = cfg[\"raw_filename\"].format(cfg[\"verbose\"], FOLD_NUM, fold_id)\n",
    "\n",
    "cfg[\"filename\"] = 'models/1M_baseline'\n",
    "\n",
    "cfg[\"num_example\"] = len(train_data)\n",
    "print(len(train_data))\n",
    "\n",
    "# tf.compat.v1.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "seed(SEED - fold_id)\n",
    "np.random.seed(SEED - fold_id)\n",
    "tf.compat.v1.random.set_random_seed(SEED - fold_id)\n",
    "train_D = data_generator(train_data)\n",
    "print(cfg)\n",
    "model = build_baseline_model(cfg, summary=True, \n",
    "                    word_embedding_matrix=word_embedding_matrix,\n",
    "                    )\n",
    "tf.keras.utils.plot_model(model, to_file=\"models/model_baseline.png\", show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OvOGGA_tP8F"
   },
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U73CNxpCtVUY"
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluate(filename=cfg[\"filename\"])\n",
    "log_dir = \"logs/fit/1M_basaline\"\n",
    "tensorboard_callback = tf.compat.v1.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "with open(\"data/val_data.pkl\", \"rb\") as f:\n",
    "    val_data = joblib.load(f)\n",
    "    \n",
    "model.fit(train_D.__iter__(),\n",
    "                          steps_per_epoch=len(train_data)//cfg[\"batch_size\"],\n",
    "                          epochs=MAX_EPOCH,\n",
    "                          callbacks=[evaluator, tensorboard_callback],\n",
    "                          shuffle=True\n",
    "                          )\n",
    "print(\"\\n\\n[!] fold_id = {} finish\".format(fold_id))\n",
    "del model, evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUo7uyS6Ar5G",
    "scrolled": true
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 73205,
     "status": "ok",
     "timestamp": 1636220046397,
     "user": {
      "displayName": "Yasser Otiefy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgH8nT7s1o73eNNJPavJleQKJePFH0HS80Pv_-tmQ=s64",
      "userId": "11606645386995589698"
     },
     "user_tz": -60
    },
    "id": "8yAEZS7dAr5G",
    "outputId": "ff7ac5dd-b2e5-4095-ef2e-f206020b4592"
   },
   "outputs": [],
   "source": [
    "with open('data/testA_data.pkl', 'rb') as outp:\n",
    "    test_data = pickle.load(outp)\n",
    "\n",
    "f = cfg[\"filename\"] + \".h5\"\n",
    "print(f)\n",
    "if \"bert\" in cfg[\"verbose\"]:\n",
    "    custom_objects = get_custom_objects()\n",
    "    model = load_model(f, custom_objects=custom_objects)  \n",
    "else:\n",
    "    model = load_model(f)\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ct2eCmYhAr5H"
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "result = defaultdict(list)\n",
    "for i in trange(len(test_data)):\n",
    "    d = test_data.iloc[i]\n",
    "    qid = d['query_id']\n",
    "    pid = d['product_id']\n",
    "    text = d['query']\n",
    "    label_words = d['label_words']\n",
    "    t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "\n",
    "    image = np.array(d['feature_convert'], dtype=\"float32\")\n",
    "    image = image[: cfg[\"max_box\"]]\n",
    "    img_mask = [1 for _ in image[: cfg[\"max_box\"]]]                   \n",
    "    pos = np.array(d['pos'], dtype=\"float32\")\n",
    "    pos = pos[: cfg[\"max_box\"]]\n",
    "\n",
    "    image_char = [token2id_X(ent, x_dict=word_index)[0] for ent in label_words.split(IMAGE_LABEM_CONCAT_TOKEN)]\n",
    "    image_char = image_char[: cfg[\"max_box\"]]\n",
    "    image_char = pad_sequences(image_char, \n",
    "                               maxlen=cfg[\"max_char\"], \n",
    "                               dtype='int32',\n",
    "                               padding='post',\n",
    "                               truncating='post',\n",
    "                               value=cfg[\"x_pad\"]) \n",
    "    output = model.predict([np.asarray([t1]), np.asarray([t2]), np.asarray([image]), np.asarray([img_mask]), np.asarray([pos]), np.asarray([image_char])])\n",
    "    result[qid].append((pid, output[0][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOWqYUaXAr5H"
   },
   "outputs": [],
   "source": [
    "query_id,product1,product2,product3,product4,product5 = [],[],[],[],[],[]\n",
    "for key in result.keys():\n",
    "    rlist = result[key]\n",
    "    rlist.sort(key=lambda x: x[1], reverse=True)\n",
    "    query_id.append(key)\n",
    "    product1.append(rlist[0][0])\n",
    "    product2.append(rlist[1][0])\n",
    "    product3.append(rlist[2][0])\n",
    "    product4.append(rlist[3][0])\n",
    "    product5.append(rlist[4][0])\n",
    "\n",
    "sub = pd.DataFrame({'query-id':query_id,\n",
    "                    'product1':product1,\n",
    "                    'product2':product2,\n",
    "                    'product3':product3,\n",
    "                    'product4':product4,\n",
    "                    'product5':product5,\n",
    "\n",
    "})\n",
    "\n",
    "sub.to_csv('result/submission_1M.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hjq_AYpAAr5I"
   },
   "outputs": [],
   "source": [
    "sub.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecQVcr2zXytP"
   },
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 630,
     "status": "ok",
     "timestamp": 1636222703249,
     "user": {
      "displayName": "Yasser Otiefy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgH8nT7s1o73eNNJPavJleQKJePFH0HS80Pv_-tmQ=s64",
      "userId": "11606645386995589698"
     },
     "user_tz": -60
    },
    "id": "RR5AV6DQXyGN",
    "outputId": "938ec73d-ba80-493f-f6c5-2b6f41fb6faf"
   },
   "outputs": [],
   "source": [
    "!python eval.py data/testA_answer.json result/submission_1M.csv result/testA_result_1M.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n",
    "fold_id = -1\n",
    "# cfg[\"filename\"] = cfg[\"raw_filename\"].format(cfg[\"verbose\"], FOLD_NUM, fold_id)\n",
    "\n",
    "cfg[\"filename\"] = 'models/100K_with_bert'\n",
    "\n",
    "cfg[\"num_example\"] = len(train_data)\n",
    "print(len(train_data))\n",
    "\n",
    "tf.compat.v1.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "seed(SEED - fold_id)\n",
    "np.random.seed(SEED - fold_id)\n",
    "tf.compat.v1.random.set_random_seed(SEED - fold_id)\n",
    "train_D = data_generator(train_data)\n",
    "print(cfg)\n",
    "model = build_model(cfg, summary=True, \n",
    "                    word_embedding_matrix=word_embedding_matrix,\n",
    "                    )\n",
    "tf.keras.utils.plot_model(model, to_file=\"models/model_with_bert.png\", show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluate(filename=cfg[\"filename\"])\n",
    "log_dir = \"logs/fit/100k_with_bert\"\n",
    "tensorboard_callback = tf.compat.v1.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "with open(\"data/val_data.pkl\", \"rb\") as f:\n",
    "    val_data = joblib.load(f)\n",
    "    \n",
    "model.fit(train_D.__iter__(),\n",
    "                          steps_per_epoch=len(train_D),\n",
    "                          epochs=MAX_EPOCH,\n",
    "                          callbacks=[evaluator, tensorboard_callback],\n",
    "                          shuffle=True\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "\n",
    "    global cfg\n",
    "    \n",
    "    def _get_model(base_dir, cfg_=None):\n",
    "        config_file = os.path.join(base_dir, 'bert_config.json')\n",
    "        checkpoint_file = os.path.join(base_dir, 'bert_model.ckpt')\n",
    "        if not os.path.exists(config_file):\n",
    "            config_file = os.path.join(base_dir, 'bert_config_large.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'roberta_l24_large_model')\n",
    "        print(config_file, checkpoint_file)\n",
    "#         model = load_trained_model_from_checkpoint(config_file, checkpoint_file, training=True, seq_len=cfg_['maxlen'])\n",
    "        model = load_trained_model_from_checkpoint(config_file, \n",
    "                                           checkpoint_file, \n",
    "                                           training=False, \n",
    "                                           trainable=cfg_[\"bert_trainable\"], \n",
    "                                           output_layer_num=cfg[\"cls_num\"],\n",
    "                                           seq_len=None)\n",
    "        return model\n",
    "    \n",
    "    def get_opt(num_example, warmup_proportion=0.1, lr=2e-5, min_lr=None):\n",
    "        if cfg[\"opt\"].lower() == \"nadam\":\n",
    "            opt = Nadam(lr=lr)\n",
    "        else:\n",
    "            total_steps, warmup_steps = calc_train_steps(\n",
    "                num_example=num_example,\n",
    "                batch_size=B_SIZE,\n",
    "                epochs=MAX_EPOCH,\n",
    "                warmup_proportion=warmup_proportion,\n",
    "            )\n",
    "\n",
    "            opt = AdamWarmup(total_steps, warmup_steps, lr=lr, min_lr=min_lr)\n",
    "\n",
    "        return opt\n",
    "\n",
    "    # model1 = _get_model(cfg[\"base_dir\"], cfg)\n",
    "    # model1 = Model(inputs=model1.inputs[: 2], outputs=model1.layers[-7].output)\n",
    "\n",
    "    global word_index\n",
    "    word_embedding_matrix = load_word_embed(word_index=word_index)\n",
    "    embed_layer = Embedding(input_dim=word_embedding_matrix.shape[0], \n",
    "                            output_dim=word_embedding_matrix.shape[1],\n",
    "                            weights=[word_embedding_matrix],\n",
    "                            trainable=cfg[\"trainable\"],\n",
    "                            name=\"embed_layer\"\n",
    "                        )\n",
    "        \n",
    "    inp_token1 = Input(shape=(None, ), dtype=np.int32, name=\"query_token_input\")\n",
    "    inp_segm1 = Input(shape=(None, ), dtype=np.float32, name=\"query_segm_input\")\n",
    "    \n",
    "#     inp_token2 = Input(shape=(None, ), dtype=np.int32)\n",
    "#     inp_segm2 = Input(shape=(None, ), dtype=np.float32)    \n",
    "    \n",
    "    inp_image = Input(shape=(None, 2048), dtype=np.float32, name=\"image_input\")\n",
    "    inp_image_mask = Input(shape=(None, ), dtype=np.float32, name=\"image_mask_input\")\n",
    "    inp_pos = Input(shape=(None, 5), dtype=np.float32, name=\"image_pos_input\")        \n",
    "    inp_image_char = Input(shape=(None, cfg[\"max_char\"]), dtype=np.int32, name='image_char_input')\n",
    "    \n",
    "    \n",
    "    mask = Lambda(lambda x: K.cast(K.not_equal(x, cfg[\"x_pad\"]), 'float32'), name=\"token_mask\")(inp_token1)\n",
    "    word_embed = embed_layer(inp_token1)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "    \n",
    "    # hp_units_lstm = hp.Int('lstm_units1', min_value=64, max_value=512, step=32)\n",
    "    word_embed = Bidirectional(LSTM(cfg[\"unit1_1\"], return_sequences=True), merge_mode=\"sum\")(word_embed)\n",
    "    word_embed = BatchNormalization()(word_embed)\n",
    "    # word_embed = Dropout(0.3)(word_embed)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "\n",
    "    # sequence_output = model1([inp_token1, inp_segm1])\n",
    "    # sequence_output = Concatenate(axis=-1)([sequence_output, word_embed])\n",
    "    text_pool = Lambda(lambda x: x[:, 0, :])(word_embed)\n",
    "\n",
    "    # Share weights of character-level embedding for premise and hypothesis\n",
    "    # hp_units_filter = hp.Int('filter_units1', min_value=64, max_value=512, step=32)\n",
    "    # hp_units_filter_size = hp.Int('filter_size1', min_value=3, max_value=12, step=2)\n",
    "    character_embedding_layer = TimeDistributed(Sequential([\n",
    "        embed_layer,\n",
    "        # Embedding(input_dim=100, output_dim=char_embedding_size, input_length=chars_per_word),\n",
    "        Conv1D(filters=128, kernel_size=3, padding='same', name=\"char_embed_conv1d\"),\n",
    "        GlobalMaxPooling1D()\n",
    "    ]), name='CharEmbedding')\n",
    "    character_embedding_layer.build(input_shape=(None, None, cfg[\"max_char\"]))\n",
    "    image_char_embed  = character_embedding_layer(inp_image_char)    \n",
    "    image_embed = Concatenate(axis=-1)([image_char_embed, inp_image])\n",
    "    # hp_units0 = hp.Int('dense_units1', min_value=256, max_value=1024, step=32)\n",
    "    hp_activation = hp.Choice('activation1', values=['relu', 'tanh', 'sigmoid', 'selu', 'elu'])\n",
    "    image_embed = Dense(256, activation=hp_activation, name='image_embed')(image_embed)\n",
    "\n",
    "    image_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([image_embed, inp_image_mask])\n",
    "\n",
    "\n",
    "    # hp_units = hp.Int('dense_units2', min_value=256, max_value=2048, step=32)\n",
    "    hp_activation = hp.Choice('activation2', values=['relu', 'tanh', 'sigmoid', 'selu', 'elu'])\n",
    "    hp_dropout_prop = hp.Choice('dropout_prop1', values=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "    pos_embed = Dense(256, activation=hp_activation, name='pos_embed')(inp_pos)\n",
    "    pos_embed = Dropout(hp_dropout_prop)(pos_embed)\n",
    "    pos_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([pos_embed, inp_image_mask])\n",
    "    embed = Add()([image_embed , pos_embed]) # batch, maxlen(10), 1024+128\n",
    "    \n",
    "    # hp_units_lstm0 = hp.Int('lstm_units1', min_value=64, max_value=512, step=32)\n",
    "    image_embed = Bidirectional(LSTM(512, return_sequences=True), merge_mode=\"sum\")(embed)\n",
    "    image_embed = BatchNormalization()(image_embed)\n",
    "    image_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([image_embed, inp_image_mask])\n",
    "    \n",
    "    image_pool = Lambda(lambda x: x[:, 0, :])(image_embed)\n",
    "    \n",
    "    pool = Concatenate(axis=-1)([image_pool, text_pool])\n",
    "\n",
    "    hp_units = hp.Int('dense_units3', min_value=64, max_value=2048, step=32)\n",
    "    hp_activation = hp.Choice('activation3', values=['relu', 'tanh', 'sigmoid', 'selu', 'elu'])\n",
    "    hp_dropout_prop = hp.Choice('dropout_prop2', values=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "    pool = Dense(hp_units, activation=hp_activation)(pool)\n",
    "    pool = Dropout(hp_dropout_prop)(pool)\n",
    "    hp_units1 = hp.Int('dense_units4', min_value=64, max_value=2048, step=32)\n",
    "    hp_activation1 = hp.Choice('activation4', values=['relu', 'tanh', 'sigmoid', 'selu', 'elu'])\n",
    "    pool = Dense(hp_units1, activation=hp_activation1)(pool)\n",
    "    hp_units2 = hp.Int('dense_units5', min_value=64, max_value=2048, step=32)\n",
    "    hp_activation2 = hp.Choice('activation5', values=['relu', 'tanh', 'sigmoid', 'selu', 'elu'])\n",
    "    pool = Dense(hp_units2, activation=hp_activation2)(pool)\n",
    "    \n",
    "    output = Dense(2, activation='softmax', name='output')(pool)\n",
    "\n",
    "    model = Model(inputs=[inp_token1, inp_segm1, \n",
    "                          inp_image, inp_image_mask,\n",
    "                          inp_pos, inp_image_char], outputs=[output])#\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss={\n",
    "                'output': 'sparse_categorical_crossentropy'\n",
    "            }, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "tf.compat.v1.random.set_random_seed(SEED)\n",
    "\n",
    "tuner = kt.RandomSearch(build_model,\n",
    "                     objective='val_accuracy',\n",
    "                     max_trials=50,\n",
    "                     directory='tmp/hyperparameter_tuning',\n",
    "                     project_name='multimodal_hyperparameter_tuning',\n",
    "                     overwrite=True,\n",
    "                     seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "log_dir = \"tmp/hparam_logs\"\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_D.__iter__(), epochs=20, validation_data=val_D.__iter__(), callbacks=[stop_early, tensorboard_callback],\n",
    "            batch_size=1024, steps_per_epoch=len(train_D), validation_steps=len(val_D))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "                     max_epochs=20,\n",
    "                     factor=3,\n",
    "                     directory='tmp/hyperband_hyperparameter_tuning',\n",
    "                     project_name='multimodal_hyperband_hyperparameter_tuning',\n",
    "                     overwrite=True,\n",
    "                     seed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "log_dir = \"tmp/hparam_hyperband_logs\"\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "tuner.search(train_D.__iter__(), epochs=20, validation_data=val_D.__iter__(), callbacks=[stop_early, tensorboard_callback],\n",
    "            batch_size=1024, steps_per_epoch=len(train_D), validation_steps=len(val_D))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir /tmp/hparam_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "                     max_epochs=20,\n",
    "                     factor=3,\n",
    "                     directory='tmp/hyperband_hyperparameter_tuning',\n",
    "                     project_name='multimodal_hyperband_hyperparameter_tuning',\n",
    "                     overwrite=False,\n",
    "                     seed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal parameters are {best_hps._hps}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperband_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "with open(\"data/val_data.pkl\", \"rb\") as f:\n",
    "    val_data = joblib.load(f)\n",
    "\n",
    "evaluator = Evaluate(filename=\"hyperband_best_model\")\n",
    "log_dir = \"logs/fit/Hyperband_best_hyparam\" \n",
    "tensorboard_callback = tf.compat.v1.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "hyperband_model.fit(train_D.__iter__(),\n",
    "                          steps_per_epoch=len(train_D),\n",
    "                          epochs=MAX_EPOCH,\n",
    "                          callbacks=[evaluator, tensorboard_callback],\n",
    "                          shuffle=True\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_models = [\"\"\"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/models/10K_with_bert_20.h5\"\"\",  \"\"\"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/models/100K_with_bert_20.h5\"\"\", \"\"\"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/models/bert_1M_example.h5\"\"\"]\n",
    "without_models = [\"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/models/10K_baseline_10.h5\", \"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/models/100K_baseline_10.h5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_data_size = [\"10K\", \"100K\", \"1M\"]\n",
    "without_data_size = [\"10K\", \"100K\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/testA_data.pkl', 'rb') as outp:\n",
    "    testA_data = pickle.load(outp)\n",
    "    \n",
    "with open('data/testB_data.pkl', 'rb') as outp:\n",
    "    testB_data = pickle.load(outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/models/10K_with_bert_20.h5 trained on 10K data\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28830 [00:00<?, ?it/s]2022-02-02 08:04:11.263644: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8301\n",
      "2022-02-02 08:04:12.110595: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-02-02 08:04:12.111422: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-02-02 08:04:12.111497: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-02-02 08:04:12.113641: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-02-02 08:04:12.114214: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "Could not load symbol cublasGetSmCountTarget from libcublas.so.11. Error: /usr/local/cuda/lib64/libcublas.so.11: undefined symbol: cublasGetSmCountTarget\n",
      "100%|██████████| 28830/28830 [1:00:52<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read standard from data/testA_answer.json\n",
      "Read user submit file from /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/bert_10K_submission_A.csv\n",
      "The evaluation finished successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29005/29005 [1:01:30<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read standard from data/testB_answer.json\n",
      "Read user submit file from /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/bert_10K_submission_B.csv\n",
      "The evaluation finished successfully.\n",
      "Evaluating model /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/models/100K_with_bert_20.h5 trained on 100K data\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28830/28830 [1:01:55<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read standard from data/testA_answer.json\n",
      "Read user submit file from /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/bert_100K_submission_A.csv\n",
      "The evaluation finished successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29005/29005 [1:01:17<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read standard from data/testB_answer.json\n",
      "Read user submit file from /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/bert_100K_submission_B.csv\n",
      "The evaluation finished successfully.\n",
      "Evaluating model /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/models/bert_1M_example.h5 trained on 1M data\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28830/28830 [1:01:13<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read standard from data/testA_answer.json\n",
      "Read user submit file from /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/bert_1M_submission_A.csv\n",
      "The evaluation finished successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29005/29005 [1:01:19<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read standard from data/testB_answer.json\n",
      "Read user submit file from /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/bert_1M_submission_B.csv\n",
      "The evaluation finished successfully.\n"
     ]
    }
   ],
   "source": [
    "bert_final_A_B_predictions = {}\n",
    "for model, size in zip(bert_models, bert_data_size):\n",
    "    print(f\"Evaluating model {model} trained on {size} data\")\n",
    "    custom_objects = get_custom_objects()\n",
    "    model = load_model(model, custom_objects=custom_objects)\n",
    "    result_A = defaultdict(list)\n",
    "    result_B = defaultdict(list)\n",
    "    for i in trange(len(testA_data)):\n",
    "        d = testA_data.iloc[i]\n",
    "        qid = d['query_id']\n",
    "        pid = d['product_id']\n",
    "        text = d['query']\n",
    "        label_words = d['label_words']\n",
    "        t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "\n",
    "        image = np.array(d['feature_convert'], dtype=\"float32\")\n",
    "        image = image[: cfg[\"max_box\"]]\n",
    "        img_mask = [1 for _ in image[: cfg[\"max_box\"]]]                   \n",
    "        pos = np.array(d['pos'], dtype=\"float32\")\n",
    "        pos = pos[: cfg[\"max_box\"]]\n",
    "\n",
    "        image_char = [token2id_X(ent, x_dict=word_index)[0] for ent in label_words.split(IMAGE_LABEM_CONCAT_TOKEN)]\n",
    "        image_char = image_char[: cfg[\"max_box\"]]\n",
    "        image_char = pad_sequences(image_char, \n",
    "                                maxlen=cfg[\"max_char\"], \n",
    "                                dtype='int32',\n",
    "                                padding='post',\n",
    "                                truncating='post',\n",
    "                                value=cfg[\"x_pad\"]) \n",
    "        output = model.predict([np.asarray([t1]), np.asarray([t2]), np.asarray([image]), np.asarray([img_mask]), np.asarray([pos]), np.asarray([image_char])])\n",
    "        result_A[qid].append((pid, output[0][1]))\n",
    "    \n",
    "    query_id,product1,product2,product3,product4,product5 = [],[],[],[],[],[]\n",
    "    for key in result_A.keys():\n",
    "        rlist = result_A[key]\n",
    "        rlist.sort(key=lambda x: x[1], reverse=True)\n",
    "        query_id.append(key)\n",
    "        product1.append(rlist[0][0])\n",
    "        product2.append(rlist[1][0])\n",
    "        product3.append(rlist[2][0])\n",
    "        product4.append(rlist[3][0])\n",
    "        product5.append(rlist[4][0])\n",
    "\n",
    "    sub = pd.DataFrame({'query-id':query_id,\n",
    "                        'product1':product1,\n",
    "                        'product2':product2,\n",
    "                        'product3':product3,\n",
    "                        'product4':product4,\n",
    "                        'product5':product5,\n",
    "\n",
    "    })\n",
    "\n",
    "    sub.to_csv(\"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/bert_\"+size+\"_submission_A.csv\", index=False)\n",
    "    os.system(\"python eval.py data/testA_answer.json /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/bert_\"+size+\"_submission_A.csv /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/bert_\"+size+\"_submission_A.json\")\n",
    "    for i in trange(len(testB_data)):\n",
    "        d = testB_data.iloc[i]\n",
    "        qid = d['query_id']\n",
    "        pid = d['product_id']\n",
    "        text = d['query']\n",
    "        label_words = d['label_words']\n",
    "        t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "\n",
    "        image = np.array(d['feature_convert'], dtype=\"float32\")\n",
    "        image = image[: cfg[\"max_box\"]]\n",
    "        img_mask = [1 for _ in image[: cfg[\"max_box\"]]]                   \n",
    "        pos = np.array(d['pos'], dtype=\"float32\")\n",
    "        pos = pos[: cfg[\"max_box\"]]\n",
    "\n",
    "        image_char = [token2id_X(ent, x_dict=word_index)[0] for ent in label_words.split(IMAGE_LABEM_CONCAT_TOKEN)]\n",
    "        image_char = image_char[: cfg[\"max_box\"]]\n",
    "        image_char = pad_sequences(image_char, \n",
    "                                maxlen=cfg[\"max_char\"], \n",
    "                                dtype='int32',\n",
    "                                padding='post',\n",
    "                                truncating='post',\n",
    "                                value=cfg[\"x_pad\"]) \n",
    "        output = model.predict([np.asarray([t1]), np.asarray([t2]), np.asarray([image]), np.asarray([img_mask]), np.asarray([pos]), np.asarray([image_char])])\n",
    "        result_B[qid].append((pid, output[0][1]))\n",
    "        \n",
    "    query_id,product1,product2,product3,product4,product5 = [],[],[],[],[],[]\n",
    "    for key in result_B.keys():\n",
    "        rlist = result_B[key]\n",
    "        rlist.sort(key=lambda x: x[1], reverse=True)\n",
    "        query_id.append(key)\n",
    "        product1.append(rlist[0][0])\n",
    "        product2.append(rlist[1][0])\n",
    "        product3.append(rlist[2][0])\n",
    "        product4.append(rlist[3][0])\n",
    "        product5.append(rlist[4][0])\n",
    "\n",
    "    sub = pd.DataFrame({'query-id':query_id,\n",
    "                        'product1':product1,\n",
    "                        'product2':product2,\n",
    "                        'product3':product3,\n",
    "                        'product4':product4,\n",
    "                        'product5':product5,\n",
    "\n",
    "    })\n",
    "\n",
    "    sub.to_csv(\"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/bert_\"+size+\"_submission_B.csv\", index=False)\n",
    "    os.system(\"python eval.py data/testB_answer.json /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/bert_\"+size+\"_submission_B.csv /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/bert_\"+size+\"_submission_B.json\")\n",
    "    bert_final_A_B_predictions[\"bert\"+size+\"_A\"] = result_A\n",
    "    bert_final_A_B_predictions[\"bert\"+size+\"_B\"] = result_B\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(bert_final_A_B_predictions, open(\"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/bert_final_A_B_predictions.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/models/10K_baseline_10.h5 trained on 10K data\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/28830 [00:01<3:48:03,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 28806 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1f343b7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/28830 [00:02<5:43:43,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 28807 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1f343b7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28830/28830 [58:18<00:00,  8.24it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read standard from data/testA_answer.json\n",
      "Read user submit file from /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/without_10K_submission_A.csv\n",
      "The evaluation finished successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29005/29005 [57:58<00:00,  8.34it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read standard from data/testB_answer.json\n",
      "Read user submit file from /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/without_10K_submission_B.csv\n",
      "The evaluation finished successfully.\n",
      "Evaluating model /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/models/100K_baseline_10.h5 trained on 100K data\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28830/28830 [53:03<00:00,  9.06it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read standard from data/testA_answer.json\n",
      "Read user submit file from /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/without_100K_submission_A.csv\n",
      "The evaluation finished successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29005/29005 [52:30<00:00,  9.21it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read standard from data/testB_answer.json\n",
      "Read user submit file from /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/without_100K_submission_B.csv\n",
      "The evaluation finished successfully.\n"
     ]
    }
   ],
   "source": [
    "without_final_A_B_predictions = {}\n",
    "for model, size in zip(without_models, without_data_size):\n",
    "    print(f\"Evaluating model {model} trained on {size} data\")\n",
    "    model = load_model(model)\n",
    "    result_A = defaultdict(list)\n",
    "    result_B = defaultdict(list)\n",
    "    for i in trange(len(testA_data)):\n",
    "        d = testA_data.iloc[i]\n",
    "        qid = d['query_id']\n",
    "        pid = d['product_id']\n",
    "        text = d['query']\n",
    "        label_words = d['label_words']\n",
    "        t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "\n",
    "        image = np.array(d['feature_convert'], dtype=\"float32\")\n",
    "        image = image[: cfg[\"max_box\"]]\n",
    "        img_mask = [1 for _ in image[: cfg[\"max_box\"]]]                   \n",
    "        pos = np.array(d['pos'], dtype=\"float32\")\n",
    "        pos = pos[: cfg[\"max_box\"]]\n",
    "\n",
    "        image_char = [token2id_X(ent, x_dict=word_index)[0] for ent in label_words.split(IMAGE_LABEM_CONCAT_TOKEN)]\n",
    "        image_char = image_char[: cfg[\"max_box\"]]\n",
    "        image_char = pad_sequences(image_char, \n",
    "                                maxlen=cfg[\"max_char\"], \n",
    "                                dtype='int32',\n",
    "                                padding='post',\n",
    "                                truncating='post',\n",
    "                                value=cfg[\"x_pad\"]) \n",
    "        output = model.predict([np.asarray([t1]), np.asarray([t2]), np.asarray([image]), np.asarray([img_mask]), np.asarray([pos]), np.asarray([image_char])])\n",
    "        result_A[qid].append((pid, output[0][1]))\n",
    "    \n",
    "    query_id,product1,product2,product3,product4,product5 = [],[],[],[],[],[]\n",
    "    for key in result_A.keys():\n",
    "        rlist = result_A[key]\n",
    "        rlist.sort(key=lambda x: x[1], reverse=True)\n",
    "        query_id.append(key)\n",
    "        product1.append(rlist[0][0])\n",
    "        product2.append(rlist[1][0])\n",
    "        product3.append(rlist[2][0])\n",
    "        product4.append(rlist[3][0])\n",
    "        product5.append(rlist[4][0])\n",
    "\n",
    "    sub = pd.DataFrame({'query-id':query_id,\n",
    "                        'product1':product1,\n",
    "                        'product2':product2,\n",
    "                        'product3':product3,\n",
    "                        'product4':product4,\n",
    "                        'product5':product5,\n",
    "\n",
    "    })\n",
    "\n",
    "    sub.to_csv(\"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/without_\"+size+\"_submission_A.csv\", index=False)\n",
    "    os.system(\"python eval.py data/testA_answer.json /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/without_\"+size+\"_submission_A.csv /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/without_\"+size+\"_submission_A.json\")\n",
    "    for i in trange(len(testB_data)):\n",
    "        d = testB_data.iloc[i]\n",
    "        qid = d['query_id']\n",
    "        pid = d['product_id']\n",
    "        text = d['query']\n",
    "        label_words = d['label_words']\n",
    "        t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "\n",
    "        image = np.array(d['feature_convert'], dtype=\"float32\")\n",
    "        image = image[: cfg[\"max_box\"]]\n",
    "        img_mask = [1 for _ in image[: cfg[\"max_box\"]]]                   \n",
    "        pos = np.array(d['pos'], dtype=\"float32\")\n",
    "        pos = pos[: cfg[\"max_box\"]]\n",
    "\n",
    "        image_char = [token2id_X(ent, x_dict=word_index)[0] for ent in label_words.split(IMAGE_LABEM_CONCAT_TOKEN)]\n",
    "        image_char = image_char[: cfg[\"max_box\"]]\n",
    "        image_char = pad_sequences(image_char, \n",
    "                                maxlen=cfg[\"max_char\"], \n",
    "                                dtype='int32',\n",
    "                                padding='post',\n",
    "                                truncating='post',\n",
    "                                value=cfg[\"x_pad\"]) \n",
    "        output = model.predict([np.asarray([t1]), np.asarray([t2]), np.asarray([image]), np.asarray([img_mask]), np.asarray([pos]), np.asarray([image_char])])\n",
    "        result_B[qid].append((pid, output[0][1]))\n",
    "        \n",
    "    query_id,product1,product2,product3,product4,product5 = [],[],[],[],[],[]\n",
    "    for key in result_B.keys():\n",
    "        rlist = result_B[key]\n",
    "        rlist.sort(key=lambda x: x[1], reverse=True)\n",
    "        query_id.append(key)\n",
    "        product1.append(rlist[0][0])\n",
    "        product2.append(rlist[1][0])\n",
    "        product3.append(rlist[2][0])\n",
    "        product4.append(rlist[3][0])\n",
    "        product5.append(rlist[4][0])\n",
    "\n",
    "    sub = pd.DataFrame({'query-id':query_id,\n",
    "                        'product1':product1,\n",
    "                        'product2':product2,\n",
    "                        'product3':product3,\n",
    "                        'product4':product4,\n",
    "                        'product5':product5,\n",
    "\n",
    "    })\n",
    "\n",
    "    sub.to_csv(\"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/without_\"+size+\"_submission_B.csv\", index=False)\n",
    "    os.system(\"python eval.py data/testB_answer.json /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/without_\"+size+\"_submission_B.csv /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/without_\"+size+\"_submission_B.json\")\n",
    "    without_final_A_B_predictions[\"without\"+size+\"_A\"] = result_A\n",
    "    without_final_A_B_predictions[\"without\"+size+\"_B\"] = result_B\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(without_final_A_B_predictions, open(\"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/without_final_A_B_predictions.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without = pickle.load(open(\"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/without_final_A_B_predictions.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = pickle.load(open(\"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/bert_final_A_B_predictions.pkl\", \"rb\"))\n",
    "without = pickle.load(open(\"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/without_final_A_B_predictions.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bert10K_A', 'bert10K_B', 'bert100K_A', 'bert100K_B', 'bert1M_A', 'bert1M_B', 'without10K_A', 'without10K_B', 'without100K_A', 'without100K_B'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models = {}\n",
    "all_models.update(bert)\n",
    "all_models.update(without)\n",
    "all_models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = {\n",
    "    # \"bert10K_B\":0.3611299223044956, \n",
    "              \"bert100K_B\":0.4322373308206258, \n",
    "            #   \"bert1M_B\":0.5444838877873618,\n",
    "            #   \"without10K_B\":0.32694372741460237, \n",
    "              \"without100K_B\":0.4189474418223332\n",
    "              } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851184772642959\n",
      "Read standard from data/testB_answer.json\n",
      "Read user submit file from /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/ensembled_B.csv\n",
      "The evaluation finished successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_weights = sum(score_dict.values())\n",
    "print(sum_weights)\n",
    "result = {}\n",
    "\n",
    "for model_key in score_dict.keys():\n",
    "    for i in all_models[model_key].keys():\n",
    "        final_score = np.zeros_like(len(all_models[model_key][i]))\n",
    "        for key in score_dict.keys():\n",
    "            sorted_prod = all_models[key][i]\n",
    "            sorted_prod.sort(key=lambda x: x[0], reverse=True)\n",
    "            score = np.array([j[1] for j in sorted_prod])\n",
    "            score = np.multiply(score_dict[key] , score) \n",
    "            final_score = np.add(final_score, score)\n",
    "        final_score /= sum_weights\n",
    "        sorted_prod = all_models[model_key][i]\n",
    "        sorted_prod.sort(key=lambda x: x[0], reverse=True)\n",
    "        result[i] = [(j[0], final_score[k]) for k, j in enumerate(sorted_prod)]\n",
    "\n",
    "query_id,product1,product2,product3,product4,product5 = [],[],[],[],[],[]\n",
    "for key in result.keys():\n",
    "    rlist = result[key]\n",
    "    rlist.sort(key=lambda x: x[1], reverse=True)\n",
    "    query_id.append(key)\n",
    "    product1.append(rlist[0][0])\n",
    "    product2.append(rlist[1][0])\n",
    "    product3.append(rlist[2][0])\n",
    "    product4.append(rlist[3][0])\n",
    "    product5.append(rlist[4][0])\n",
    "\n",
    "sub = pd.DataFrame({'query-id':query_id,\n",
    "                    'product1':product1,\n",
    "                    'product2':product2,\n",
    "                    'product3':product3,\n",
    "                    'product4':product4,\n",
    "                    'product5':product5,\n",
    "\n",
    "})\n",
    "\n",
    "sub.to_csv(\"/root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/ensembled_B.csv\", index=False)\n",
    "os.system(\"python eval.py data/testB_answer.json /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/ensembled_B.csv /root/Applied_AI_Lab_WiSe2021_Passau/ai-light/final_results/ensemble_submission_B.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "[image-concat-query]-wwm_uncased_L12-768_v3_quart.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
